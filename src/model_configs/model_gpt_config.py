import torch
from transformers.models.gpt_neo.configuration_gpt_neo import GPTNeoConfig
import configs


class GPTNeoXAIConfig(GPTNeoConfig):
    def __init__(self, vocab_size=50257, max_position_embeddings=2048, hidden_size=768, num_layers=12, num_heads=12,
                 intermediate_size=None, window_size=256, activation_function="gelu_new", resid_dropout=0,
                 embed_dropout=0, attention_dropout=0, layer_norm_epsilon=1e-05, initializer_range=0.02,
                 summary_type="cls_index", summary_use_proj=True, summary_activation=None, summary_first_dropout=0.1,
                 summary_proj_to_labels=True, use_cache=True, bos_token_id=50256, eos_token_id=50256,
                 attention_types=[[["global", "local"], 6]], return_dict=True, output_hidden_states=False,
                 output_attentions=False, torchscript=False, torch_dtype=torch.float32, use_bfloat16=False,
                 tf_legacy_loss=False, pruned_heads={}, tie_word_embeddings=True, is_encoder_decoder=False,
                 is_decoder=False, cross_attention_hidden_size=None, add_cross_attention=False,
                 tie_encoder_decoder=False, max_length=20, min_length=0, do_sample=False, early_stopping=False,
                 num_beams=1, num_beam_groups=1, diversity_penalty=0.0, temperature=1.0, top_k=50, top_p=1.0,
                 typical_p=1.0, repetition_penalty=1.0, length_penalty=1.0, no_repeat_ngram_size=0,
                 encoder_no_repeat_ngram_size=0, bad_words_ids=None, num_return_sequences=1, chunk_size_feed_forward=0,
                 output_scores=False, return_dict_in_generate=False, forced_bos_token_id=None, forced_eos_token_id=None,
                 remove_invalid_values=False, exponential_decay_length_penalty=None, suppress_tokens=None,
                 begin_suppress_tokens=None, finetuning_task=None, tokenizer_class=None, prefix=None, pad_token_id=None,
                 sep_token_id=None, decoder_start_token_id=None, task_specific_params=None, problem_type=None,
                 gradient_checkpointing=False, train_mode=False, device=configs.device, **kwargs):

        super().__init__(vocab_size=vocab_size,
                         max_position_embeddings=max_position_embeddings,
                         hidden_size=hidden_size,
                         num_layers=num_layers,
                         num_heads=num_heads,
                         intermediate_size=intermediate_size,
                         window_size=window_size,
                         activation_function=activation_function,
                         resid_dropout=resid_dropout,
                         embed_dropout=embed_dropout,
                         attention_dropout=attention_dropout,
                         layer_norm_epsilon=layer_norm_epsilon,
                         initializer_range=initializer_range,
                         summary_type=summary_type,
                         summary_use_proj=summary_use_proj,
                         summary_activation=summary_activation,
                         summary_first_dropout=summary_first_dropout,
                         summary_proj_to_labels=summary_proj_to_labels,
                         use_cache=use_cache,
                         bos_token_id=bos_token_id,
                         eos_token_id=eos_token_id,
                         attention_types=attention_types,

                         return_dict=return_dict,
                         output_hidden_states=output_hidden_states,
                         output_attentions=output_attentions,
                         torchscript=torchscript,
                         torch_dtype=torch_dtype,
                         use_bfloat16=use_bfloat16,
                         tf_legacy_loss=tf_legacy_loss,
                         pruned_heads=pruned_heads,
                         tie_word_embeddings=tie_word_embeddings,
                         is_encoder_decoder=is_encoder_decoder,
                         is_decoder=is_decoder,
                         cross_attention_hidden_size=cross_attention_hidden_size,
                         add_cross_attention=add_cross_attention,
                         tie_encoder_decoder=tie_encoder_decoder,
                         max_length=max_length,
                         min_length=min_length,
                         do_sample=do_sample,
                         early_stopping=early_stopping,
                         num_beams=num_beams,
                         num_beam_groups=num_beam_groups,
                         diversity_penalty=diversity_penalty,
                         temperature=temperature,
                         top_k=top_k,
                         top_p=top_p,
                         typical_p=typical_p,
                         repetition_penalty=repetition_penalty,
                         length_penalty=length_penalty,
                         no_repeat_ngram_size=no_repeat_ngram_size,
                         encoder_no_repeat_ngram_size=encoder_no_repeat_ngram_size,
                         bad_words_ids=bad_words_ids,
                         num_return_sequences=num_return_sequences,
                         chunk_size_feed_forward=chunk_size_feed_forward,
                         output_scores=output_scores,
                         return_dict_in_generate=return_dict_in_generate,
                         forced_bos_token_id=forced_bos_token_id,
                         forced_eos_token_id=forced_eos_token_id,
                         remove_invalid_values=remove_invalid_values,
                         exponential_decay_length_penalty=exponential_decay_length_penalty,
                         suppress_tokens=suppress_tokens,
                         begin_suppress_tokens=begin_suppress_tokens,
                         finetuning_task=finetuning_task,
                         tokenizer_class=tokenizer_class,
                         prefix=prefix,
                         pad_token_id=pad_token_id,
                         sep_token_id=sep_token_id,
                         decoder_start_token_id=decoder_start_token_id,
                         task_specific_params=task_specific_params,
                         problem_type=problem_type,
                         gradient_checkpointing=gradient_checkpointing)
        self.train_mode = train_mode
        self.devide = device
